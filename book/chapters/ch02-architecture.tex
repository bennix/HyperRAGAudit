\chapter{系统架构设计}
\label{ch:architecture}

\section{总体架构}

HyperRAG 采用分层管道（Pipeline）架构，数据从文档输入到审计报告输出，经过六个核心阶段：

\begin{center}
\begin{tikzpicture}[
  node distance=1.2cm,
  block/.style={rectangle, draw, rounded corners, fill=blue!8,
    minimum width=5cm, minimum height=1cm, align=center,
    font=\small},
  arrow/.style={-{Stealth[length=3mm]}, thick, color=chaptercolor}
]
  \node[block] (input) {文档输入层\\PDF / Image / DOCX};
  \node[block, below=of input] (convert) {文档转换层\\DocConverter};
  \node[block, below=of convert] (ocr) {OCR 解析层\\GeminiParser (Vision LLM)};
  \node[block, below=of ocr] (storage) {存储层\\VectorStore + KnowledgeGraphStore};
  \node[block, below=of storage] (agent) {推理层\\AuditAgent (LangGraph ReAct)};
  \node[block, below=of agent] (output) {输出层\\AuditReport + PDFHighlighter};

  \draw[arrow] (input) -- (convert);
  \draw[arrow] (convert) -- (ocr);
  \draw[arrow] (ocr) -- (storage);
  \draw[arrow] (storage) -- (agent);
  \draw[arrow] (agent) -- (output);
\end{tikzpicture}
\end{center}

\section{核心组件清单}

系统由以下核心组件构成：

\begin{table}[htbp]
\centering
\caption{HyperRAG 核心组件}
\label{tab:components}
\begin{tabular}{llp{7cm}}
\toprule
\textbf{组件} & \textbf{文件} & \textbf{职责} \\
\midrule
DocConverter & \texttt{parser/doc\_converter.py} & 将 PDF/图片/DOCX 转换为 JPEG 页面图片 \\
GeminiParser & \texttt{parser/gemini\_parser.py} & 调用 Vision LLM 进行 OCR，返回带 BBox 的结构化内容 \\
VectorStore & \texttt{graph/vector\_store.py} & ChromaDB 向量存储与语义检索 \\
EntityExtractor & \texttt{graph/entity\_extractor.py} & 调用 Claude 提取实体与关系 \\
KGStore & \texttt{graph/kg\_store.py} & NetworkX 知识图谱的增删查改 \\
AuditAgent & \texttt{agent/audit\_agent.py} & LangGraph ReAct Agent，协调多工具推理 \\
Tools & \texttt{agent/tools.py} & 5 个 LangChain Tool 供 Agent 使用 \\
PDFHighlighter & \texttt{highlighter/pdf\_highlighter.py} & BBox 坐标转换 + PDF 高亮标注 \\
LLMClientFactory & \texttt{llm/client.py} & 创建 OpenAI 客户端和 LangChain LLM \\
Settings & \texttt{config/settings.py} & 加载 \texttt{config.yaml} 配置 \\
\bottomrule
\end{tabular}
\end{table}

\section{数据模型设计}

系统的核心数据流通过一组 Pydantic\index{Pydantic} 模型定义，位于 \texttt{hyperrag/models/schemas.py}：

\subsection{边界框（BBox）}

BBox 是整个系统的核心数据结构之一，它贯穿从 OCR 解析到 PDF 高亮的完整链路：

\begin{lstlisting}[caption={BBox 数据模型}]
class BBox(BaseModel):
    """归一化坐标 [y_min, x_min, y_max, x_max]，范围 0-1000"""
    y_min: int = Field(ge=0, le=1000)
    x_min: int = Field(ge=0, le=1000)
    y_max: int = Field(ge=0, le=1000)
    x_max: int = Field(ge=0, le=1000)
\end{lstlisting}

\begin{notebox}[为什么使用 0-1000 归一化坐标？]
Vision LLM（如 Gemini、GPT）返回的坐标是相对于图片尺寸的归一化值。使用 0-1000 整数范围（而非 0.0-1.0 浮点数）可以：
\begin{itemize}
  \item 避免浮点精度问题
  \item 提供足够的空间分辨率（1/1000 的精度对 A4 页面约为 0.2mm）
  \item 简化 JSON 序列化（整数比浮点数更紧凑）
\end{itemize}
\end{notebox}

\subsection{内容块（ContentBlock）}

每个页面的内容被拆分为多个内容块，每个块携带类型、文本和坐标：

\begin{lstlisting}[caption={ContentBlock 数据模型}]
class ContentType(str, Enum):
    TEXT = "text"
    TABLE = "table"
    FIGURE = "figure"

class ContentBlock(BaseModel):
    content_type: ContentType
    text: str
    bbox: BBox
    confidence: Optional[float] = None
\end{lstlisting}

\subsection{数据流转关系}

\begin{center}
\begin{tikzpicture}[
  node distance=0.8cm and 2cm,
  model/.style={rectangle, draw, rounded corners, fill=green!8,
    minimum width=3cm, minimum height=0.7cm, align=center, font=\footnotesize},
  arrow/.style={-{Stealth[length=2mm]}, thick}
]
  \node[model] (pi) {PageImage};
  \node[model, right=of pi] (cb) {ContentBlock + BBox};
  \node[model, right=of cb] (pd) {ParsedDocument};
  \node[model, below=of cb] (ent) {Entity + Relation};
  \node[model, below=of pd] (vec) {VectorStore (ChromaDB)};
  \node[model, below=of ent] (kg) {KnowledgeGraphStore};
  \node[model, below right=1cm and 0cm of kg] (af) {AuditFinding};
  \node[model, right=of af] (pr) {PDFRect};

  \draw[arrow] (pi) -- node[above,font=\tiny]{OCR} (cb);
  \draw[arrow] (cb) -- (pd);
  \draw[arrow] (pd) -- (vec);
  \draw[arrow] (cb) -- node[left,font=\tiny]{实体提取} (ent);
  \draw[arrow] (ent) -- (kg);
  \draw[arrow] (vec) -- (af);
  \draw[arrow] (kg) -- (af);
  \draw[arrow] (af) -- node[above,font=\tiny]{坐标转换} (pr);
\end{tikzpicture}
\end{center}

\section{技术栈选型}

\begin{table}[htbp]
\centering
\caption{技术栈选型与理由}
\label{tab:tech-stack}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{类别} & \textbf{选择} & \textbf{理由} \\
\midrule
LLM 网关 & ZenMux & OpenAI 兼容接口，一个 Key 调用多模型，详见 \url{https://zenmux.ai/invite/GBQMC5} \\
Vision OCR & GPT-5.2 / Gemini & 支持图片输入，能输出结构化 JSON + BBox \\
Agent 推理 & Claude Sonnet & 强逻辑推理能力，适合多步审计分析 \\
Agent 框架 & LangGraph & 原生支持 ReAct 模式，流式输出 \\
向量数据库 & ChromaDB & 嵌入式、零配置、持久化存储 \\
知识图谱 & NetworkX & 轻量级、纯 Python、适合单用户场景 \\
PDF 处理 & PyMuPDF & 高性能 PDF 渲染与标注 \\
Web 框架 & Streamlit & 快速构建数据应用，原生 Python \\
数据验证 & Pydantic & 类型安全、自动序列化 \\
配置管理 & PyYAML & 人类可读的配置格式 \\
\bottomrule
\end{tabular}
\end{table}

\section{项目目录结构}

\begin{lstlisting}[language={},basicstyle=\small\ttfamily,frame=none,numbers=none,
  caption={HyperRAG 项目目录}]
HyperRAG/
|-- app.py                    # Streamlit 入口
|-- config.yaml               # 运行时配置（gitignored）
|-- config.yaml.example       # 配置模板
|-- requirements.txt
|-- config/
|   `-- settings.py           # YAML 配置加载器
|-- hyperrag/
|   |-- models/schemas.py     # 全部 Pydantic 数据模型
|   |-- parser/
|   |   |-- doc_converter.py  # 文档 -> JPEG 页面
|   |   `-- gemini_parser.py  # Vision LLM OCR
|   |-- graph/
|   |   |-- vector_store.py   # ChromaDB 向量存储
|   |   |-- entity_extractor.py  # 实体关系提取
|   |   `-- kg_store.py       # NetworkX 知识图谱
|   |-- agent/
|   |   |-- audit_agent.py    # LangGraph ReAct Agent
|   |   `-- tools.py          # 5 个 Agent 工具
|   |-- highlighter/
|   |   `-- pdf_highlighter.py  # PDF 高亮标注
|   `-- llm/client.py         # LLM 客户端工厂
|-- prompts/                  # 提示词模板
`-- data/                     # 运行时数据（gitignored）
\end{lstlisting}

\section{API 调用模式}

系统中存在两种不同的 LLM 调用模式：

\subsection{原始 OpenAI Client（用于 Vision OCR）}

Vision API 需要精确控制 base64 图片消息格式，因此使用原始 \texttt{openai.OpenAI} 客户端：

\begin{lstlisting}[caption={Vision API 调用}]
from openai import OpenAI

client = OpenAI(
    api_key="sk-...",
    base_url="https://zenmux.ai/api/v1",
)

resp = client.chat.completions.create(
    model="openai/gpt-5.2-chat",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [
            {"type": "text", "text": "Extract content..."},
            {"type": "image_url", "image_url": {
                "url": f"data:image/jpeg;base64,{b64_image}"
            }},
        ]},
    ],
    max_tokens=4096,
    temperature=0.0,
)
\end{lstlisting}

\subsection{LangChain ChatOpenAI（用于 Agent）}

LangGraph 的 \texttt{create\_react\_agent} 要求传入 LangChain 的 \texttt{BaseChatModel}，因此 Agent 使用 \texttt{ChatOpenAI} 包装器：

\begin{lstlisting}[caption={LangChain LLM 初始化}]
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="anthropic/claude-sonnet-4",
    api_key="sk-...",
    base_url="https://zenmux.ai/api/v1",
    temperature=0.0,
)
\end{lstlisting}

\begin{tipbox}[设计启示]
同一个 ZenMux API Key 和 base\_url 同时服务于两种调用模式，体现了统一 API 网关的价值——不同的客户端库、不同的模型供应商，都通过同一个端点完成调用。
\end{tipbox}
