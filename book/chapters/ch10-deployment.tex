\chapter{配置管理与部署}
\label{ch:deployment}

\section{配置管理}

\subsection{为什么需要外部配置}

将 API Key、模型名称、路径等参数硬编码在代码中会带来诸多问题：

\begin{enumerate}
  \item \textbf{安全风险}——API Key 可能泄露到版本控制系统
  \item \textbf{环境差异}——开发环境和生产环境需要不同的配置
  \item \textbf{灵活性差}——切换模型或调整参数需要修改代码
\end{enumerate}

HyperRAG 使用 \texttt{config.yaml} 作为外部配置文件，通过 \texttt{.gitignore} 排除，确保敏感信息不进入代码仓库。

\subsection{YAML 配置结构}

\begin{lstlisting}[language=yaml,caption={config.yaml 完整结构}]
# ZenMux API 设置
# 注册获取 API Key: https://zenmux.ai/invite/GBQMC5
api:
  base_url: "https://zenmux.ai/api/v1"
  api_key: "sk-your-api-key-here"

# 模型配置
models:
  ocr_model: "openai/gpt-5.2-chat"
  agent_model: "anthropic/claude-sonnet-4"

# 解析器设置
parser:
  dpi: 150
  jpeg_quality: 75
  max_tokens: 4096

# 向量数据库设置
vectordb:
  persist_dir: "data/chroma_db"
  collection_name: "hyperrag_docs"

# Agent 设置
agent:
  max_iterations: 10

# 数据目录
paths:
  upload_dir: "data/uploads"
  parsed_dir: "data/parsed"
  highlighted_dir: "data/highlighted"
  prompts_dir: "prompts"
\end{lstlisting}

\subsection{配置加载器}

\texttt{Settings} 类从 YAML 文件加载所有配置，并将相对路径解析为绝对路径：

\begin{lstlisting}[caption={Settings 配置加载器}]
import yaml
from pathlib import Path

_PROJECT_ROOT = Path(__file__).resolve().parent.parent
_CONFIG_PATH = _PROJECT_ROOT / "config.yaml"

class Settings:
    def __init__(self, config_path=None):
        path = Path(config_path) if config_path else _CONFIG_PATH
        if not path.exists():
            raise FileNotFoundError(
                f"Config file not found: {path}"
            )

        with open(path, encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        # API
        api = cfg.get("api", {})
        self.zenmux_base_url = api.get(
            "base_url", "https://zenmux.ai/api/v1"
        )
        self.zenmux_api_key = api.get("api_key", "")

        # Models
        models = cfg.get("models", {})
        self.gemini_model = models.get(
            "ocr_model", "openai/gpt-5.2-chat"
        )
        self.claude_model = models.get(
            "agent_model", "anthropic/claude-sonnet-4"
        )

        # Paths -- resolve relative to project root
        paths = cfg.get("paths", {})
        self.upload_dir = str(
            _PROJECT_ROOT / paths.get("upload_dir", "data/uploads")
        )
        ...
\end{lstlisting}

\begin{tipbox}[yaml.safe\_load 与 yaml.load]
始终使用 \texttt{yaml.safe\_load()} 而非 \texttt{yaml.load()}。后者允许执行任意 Python 代码（通过 YAML 的 \texttt{!!python/object} 标签），存在代码注入风险。
\end{tipbox}

\subsection{配置模板}

仓库中包含 \texttt{config.yaml.example} 作为模板，用户复制后填入自己的 API Key：

\begin{lstlisting}[language={},basicstyle=\small\ttfamily,frame=none,numbers=none]
$ cp config.yaml.example config.yaml
$ vim config.yaml  # 填入 API Key
\end{lstlisting}

\section{LLM 客户端工厂}

\texttt{LLMClientFactory} 封装了 LLM 客户端的创建逻辑，处理超时、重试等网络配置：

\begin{lstlisting}[caption={LLM 客户端工厂}]
import httpx
from openai import OpenAI
from langchain_openai import ChatOpenAI

class LLMClientFactory:
    def __init__(self, api_key: str, base_url: str):
        self._api_key = api_key
        self._base_url = base_url

    def get_openai_client(self) -> OpenAI:
        """Raw OpenAI client for Vision API."""
        return OpenAI(
            api_key=self._api_key,
            base_url=self._base_url,
            timeout=httpx.Timeout(300.0, connect=30.0),
            max_retries=3,
        )

    def get_langchain_llm(self, model: str) -> ChatOpenAI:
        """LangChain wrapper for Agent."""
        return ChatOpenAI(
            model=model,
            api_key=self._api_key,
            base_url=self._base_url,
            temperature=0.0,
        )
\end{lstlisting}

\subsection{超时与重试策略}

\begin{description}
  \item[连接超时 30s] 如果 30 秒内无法建立 TCP 连接，认为网络不可达。
  \item[总超时 300s] Vision OCR 对大页面的处理可能需要较长时间，5 分钟是合理上限。
  \item[最大重试 3 次] 网络波动导致的临时失败会自动重试。
\end{description}

\section{ZenMux API 网关}

HyperRAG 的所有 LLM 调用都通过 ZenMux\index{ZenMux} 统一 API 网关完成。

\subsection{注册与获取 API Key}

\begin{enumerate}
  \item 访问 \url{https://zenmux.ai/invite/GBQMC5}
  \item 注册账户
  \item 在控制台获取 API Key（格式：\texttt{sk-...}）
  \item 将 API Key 填入 \texttt{config.yaml}
\end{enumerate}

\subsection{模型命名规范}

ZenMux 的模型名称格式为 \texttt{provider/model-name}：

\begin{table}[htbp]
\centering
\caption{ZenMux 可用模型示例}
\begin{tabular}{lll}
\toprule
\textbf{模型名称} & \textbf{供应商} & \textbf{用途} \\
\midrule
\texttt{openai/gpt-5.2-chat} & OpenAI & Vision OCR \\
\texttt{anthropic/claude-sonnet-4} & Anthropic & Agent 推理 \\
\texttt{google/gemini-3-pro-preview} & Google & Vision OCR（备选） \\
\texttt{anthropic/claude-opus-4} & Anthropic & 复杂推理 \\
\bottomrule
\end{tabular}
\end{table}

切换模型只需修改 \texttt{config.yaml}，无需改动任何代码。

\section{目录结构与 .gitignore}

\subsection{运行时数据目录}

\begin{lstlisting}[language={},basicstyle=\small\ttfamily,frame=none,numbers=none,
  caption={data/ 目录结构}]
data/
|-- uploads/       # 用户上传的原始文件
|-- parsed/        # OCR 解析结果 (JSON)
|-- highlighted/   # 高亮标注后的 PDF
`-- chroma_db/     # ChromaDB 持久化数据
\end{lstlisting}

所有 \texttt{data/} 子目录都被 \texttt{.gitignore} 排除。

\subsection{.gitignore 策略}

\begin{lstlisting}[language={},caption={.gitignore},basicstyle=\small\ttfamily,frame=none,numbers=none]
.env                # 环境变量（备用）
config.yaml         # 包含 API Key 的配置文件
__pycache__/        # Python 字节码缓存
*.pyc
data/uploads/       # 用户数据
data/parsed/
data/highlighted/
data/chroma_db/     # 向量数据库
.venv/              # 虚拟环境
*.egg-info/
dist/
build/
\end{lstlisting}

\section{生产环境部署建议}

\subsection{Docker 部署}

\begin{lstlisting}[language={},caption={Dockerfile 示例},basicstyle=\small\ttfamily]
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app.py", \
     "--server.port=8501", \
     "--server.address=0.0.0.0"]
\end{lstlisting}

\subsection{环境变量覆盖}

在容器化部署中，可以通过环境变量覆盖 \texttt{config.yaml} 中的敏感配置，只需修改 \texttt{Settings} 类添加环境变量优先级：

\begin{lstlisting}[caption={环境变量优先级}]
import os

self.zenmux_api_key = os.getenv(
    "ZENMUX_API_KEY",
    api.get("api_key", ""),
)
\end{lstlisting}

\subsection{性能考虑}

\begin{enumerate}
  \item \textbf{并发}——Streamlit 原生是单线程的。对于多用户场景，考虑部署多个实例并使用负载均衡。
  \item \textbf{ChromaDB}——嵌入式模式只支持单进程访问。多实例部署时需使用 ChromaDB Server 模式或切换到 Qdrant 等客户端-服务器架构的向量数据库。
  \item \textbf{NetworkX}——内存中的图数据在进程重启后丢失。生产环境应考虑将图序列化到磁盘或使用 Neo4j。
  \item \textbf{文件存储}——\texttt{data/} 目录应挂载为持久化卷（Docker Volume），避免容器重启后数据丢失。
\end{enumerate}

\section{扩展方向}

HyperRAG 作为一个教学项目，展示了 RAG 审计系统的核心架构。在生产化过程中，可以考虑以下扩展：

\begin{enumerate}
  \item \textbf{异步 OCR}——使用 Celery 或 asyncio 并行处理多个页面的 OCR，大幅缩短处理时间。
  \item \textbf{增量更新}——当文档更新时，只重新解析和索引变化的页面，而非整份文档。
  \item \textbf{用户认证}——集成 OAuth 或 LDAP，支持多用户隔离。
  \item \textbf{审计模板}——预定义常见审计场景的查询模板，降低使用门槛。
  \item \textbf{导出功能}——将审计报告导出为 PDF 或 Excel，便于分发。
  \item \textbf{多模型对比}——同时调用多个 LLM 进行 OCR，取共识结果以提高准确率。
\end{enumerate}

\begin{notebox}[总结]
本书从零开始，系统地介绍了构建企业级 RAG 审计系统所需的全部知识：文档处理、Vision OCR、向量检索、知识图谱、Agent 推理、PDF 溯源高亮、Web 界面以及配置部署。每个章节都结合 HyperRAG 的实际代码进行讲解，读者可以对照源码加深理解。

完整源码：\url{https://github.com/bennix/HyperRAGAudit}

LLM API：\url{https://zenmux.ai/invite/GBQMC5}
\end{notebox}
